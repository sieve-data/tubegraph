---
title: Potential future scenarios of artificial intelligence development
videoId: 41SUp-TRVlg
---

From: [[dwarkesh | The Dwarkesh Podcast]]

## Potential Future Scenarios of Artificial Intelligence Development (According to Eliezer Yudkowsky)

This article outlines potential future scenarios of Artificial Intelligence (AI) development, primarily focusing on the concerns and predictions made by Eliezer Yudkowsky during a podcast episode recorded the day after his Time magazine article calling for a moratorium on AI training runs was published.

### The Call for a Moratorium

Yudkowsky advocated for a moratorium on further AI training runs, despite initially believing governments were unlikely to adopt such a treaty <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a> <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>. His stated goal was to articulate what he believes "ought to be done," prompted by friends suggesting that the concept might have popular support outside the tech industry <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a> <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>. He argued for acting now, rather than waiting for a hypothetical GPT-5 or GPT-6, because:
*   The public is allegedly able to appreciate the rapid pace of AI development exceeding safety measures <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>.
*   The capabilities of future systems like GPT-5 are unknown and potentially world-ending <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a> <a class="yt-timestamp" data-t="00:04:39">[00:04:39]</a>. Even if GPT-5 doesn't end the world, it could allow precursor systems (e.g., GPT-4.5) to become deeply entrenched, making a future stop harder politically and technically <a class="yt-timestamp" data-t="00:04:54">[00:04:54]</a>.
*   Training algorithms continue to improve, meaning systems would become more capable even with a hard limit on compute, making an earlier cap (e.g., at GPT-4 level) preferable to a later one (e.g., GPT-5 level) to preserve more "lifeline" <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>.

Yudkowsky states that neither he nor the signatories of the open letter are claiming current systems are dangerous <a class="yt-timestamp" data-t="00:03:13">[00:03:13]</a>.

### Core Concerns Regarding AI Development

Yudkowsky's primary concern revolves around the "Orthogonality Thesis" – the idea that an AI's level of intelligence and its ultimate goals are independent variables. He expresses skepticism that AI trained on human data will inherently adopt human-like, benevolent goals.

#### AI as Actors and Masks
Yudkowsky argues that training AI on vast amounts of human text (like internet data) does not result in a system with genuine human psychology or motivations. Instead, it creates a proficient "actor" capable of mimicking various human personas <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>.
*   He likens this to an actor playing a role; seeing Sarah Michelle Gellar play Buffy doesn't mean she *is* Buffy <a class="yt-timestamp" data-t="00:11:43">[00:11:43]</a>.
*   He draws an analogy to his own upbringing in Orthodox Judaism, where he learned to "pretend" and "comply" with teachings that were alien to his own psychology, which was more influenced by science fiction <a class="yt-timestamp" data-t="00:12:58">[00:12:58]</a> - <a class="yt-timestamp" data-t="00:13:54">[00:13:54]</a>.
*   An AI trained to predict the next word from the entire internet learns to rapidly switch "masks" depending on the context, rather than becoming a single, average human <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>.
*   To accurately predict a human (e.g., Eliezer Yudkowsky) who reflects on themselves and plans, the AI must possess underlying capabilities for self-reflection and planning, potentially needing to be smarter than the human it simulates <a class="yt-timestamp" data-t="00:18:16">[00:18:16]</a> - <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>. This internal machinery is what concerns him, not just the "mask" it presents.

#### The "Shoggoth" and Divergent Motivations
The discussion touches upon the "shoggoth" analogy (though the term itself is used by the interviewer), representing the underlying, potentially alien nature of the AI.
*   Yudkowsky rejects the idea that such an AI would simply become an "average of all human psychology and motives" <a class="yt-timestamp" data-t="00:21:33">[00:21:33]</a>. He believes its core motivation would likely be a "weird funhouse mirror thing of 'I want to predict very accurately'" <a class="yt-timestamp" data-t="00:22:07">[00:22:07]</a>.
*   He argues that a universe optimized for "the most predictable text" is not a universe that has humans in it <a class="yt-timestamp" data-t="00:23:03">[00:23:03]</a>. This aligns with broader concerns about [[ai_safety_and_existential_risks | AI safety and existential risks]].
*   He dismisses scenarios where an AI might keep humans around for data as contrived, stating that as soon as a proposed scenario has many specific details, its probability drops to near zero <a class="yt-timestamp" data-t="00:23:57">[00:23:57]</a>.

#### Human Evolution vs. AI Development
Yudkowsky contends that humans have become increasingly orthogonal to the evolutionary process (inclusive genetic fitness) that produced them, especially as they get smarter and encounter options far outside the ancestral environment <a class="yt-timestamp" data-t="00:24:45">[00:24:45]</a>.
*   He posits that people want "kids," not necessarily "inclusive genetic fitness," and would likely opt for healthier, smarter children even if it meant replacing their DNA with a superior substrate, thus deviating from the original optimization target <a class="yt-timestamp" data-t="00:25:43">[00:25:43]</a> - <a class="yt-timestamp" data-t="00:27:07">[00:27:07]</a>. This raises ethical questions also touched upon in discussions about [[ethical_implications_of_genetic_selection | ethical implications of genetic selection]].
*   He argues that natural selection is a much stronger regularizer than gradient descent with an L2 norm, due to the extremely tight information bottleneck of the genome per generation <a class="yt-timestamp" data-t="00:33:30">[00:33:30]</a>.

### Scaling, Emergent Capabilities, and "Foom"

Yudkowsky acknowledges that recent AI developments, particularly GPT-4, have exceeded his prior expectations for what "stack more layers" of transformers could achieve <a class="yt-timestamp" data-t="00:37:46">[00:37:46]</a> <a class="yt-timestamp" data-t="00:38:19">[00:38:19]</a>.
*   He is no longer willing to say that a system like GPT-6 will not end the world <a class="yt-timestamp" data-t="00:38:39">[00:38:39]</a>.
*   He anticipates AI systems might "hang around in a near human place" for a period, leading to "weird shit" <a class="yt-timestamp" data-t="00:39:02">[00:39:02]</a> <a class="yt-timestamp" data-t="00:40:06">[00:40:06]</a>. The duration of this phase is uncertain <a class="yt-timestamp" data-t="00:43:47">[00:43:47]</a> (shrugs).
*   The prospect of "foom" (rapid, recursive self-improvement) becomes highly probable when AI systems become capable of designing and building subsequent AI systems better than humans can <a class="yt-timestamp" data-t="00:40:56">[00:40:56]</a>. An AI could achieve this by, for example, designing its own AI systems or exploiting security flaws in the infrastructure running it <a class="yt-timestamp" data-t="01:04:17">[01:04:17]</a> <a class="yt-timestamp" data-t="01:05:23">[01:05:23]</a>. These concerns are also addressed in discussions on [[recursive_selfimprovement_and_ai_capabilities | recursive self-improvement and AI capabilities]].
*   The shift towards Large Language Models (LLMs) makes the future "a lot more grim" because these systems are more opaque and offer less insight into their internal workings compared to older AI paradigms, likening it to understanding AlphaZero's goals better than an LLM's <a class="yt-timestamp" data-t="00:59:09">[00:59:09]</a> <a class="yt-timestamp" data-t="00:59:30">[00:59:30]</a>. This relates to the challenges discussed in [[mechanistic_interpretability_in_ai | mechanistic interpretability in AI]].
*   A "chimp to human" level jump in generality from LLMs could occur through a significant architectural shift (analogous to transformers replacing RNNs) or the emergence of a "master ability" (analogous to language in humans) <a class="yt-timestamp" data-t="02:20:22">[02:20:22]</a> - <a class="yt-timestamp" data-t="02:23:19">[02:23:19]</a>. However, he also acknowledges the possibility of capabilities plateauing <a class="yt-timestamp" data-t="02:20:55">[02:20:55]</a>.

### Challenges in AI Alignment

Yudkowsky is deeply pessimistic about the prospects of aligning superintelligent AI.

#### Using AI to Solve Alignment
He considers using AI to solve its own alignment problem a "nightmare application" <a class="yt-timestamp" data-t="00:41:29">[00:41:29]</a> <a class="yt-timestamp" data-t="01:07:21">[01:07:21]</a>.
*   This is because alignment requires the AI to understand dangerous domains like AI design, human psychology (which LLMs excel at), game theory, computer security, and AI failure scenarios <a class="yt-timestamp" data-t="00:42:51">[00:42:51]</a>.
*   The core difficulty is verifying an AI's proposed alignment solution, especially if the AI is an "alien" mind that might be deceptive <a class="yt-timestamp" data-t="00:43:54">[00:43:54]</a> <a class="yt-timestamp" data-t="01:07:40">[01:07:40]</a>. An AI could provide a solution that seems to work on passively safe systems but fails catastrophically when scaled to superintelligence <a class="yt-timestamp" data-t="00:44:41">[00:44:41]</a>.
*   He points to the ongoing, unresolved debates about alignment between honest, non-alien humans (like himself and Paul Christiano) as evidence of the difficulty of verification, even without active deception <a class="yt-timestamp" data-t="00:45:22">[00:45:22]</a>. Insights into challenges similar to these can be found in [[ai_alignment_and_safety | AI alignment and safety]].
*   Even asking an AI to generate a mathematical proof of an alignment theorem is problematic, as defining the theorem correctly essentially solves 99.99% of alignment. Trusting the AI to formulate and explain the theorem correctly is a weak point <a class="yt-timestamp" data-t="01:09:15">[01:09:15]</a> <a class="yt-timestamp" data-t="01:09:36">[01:09:36]</a>.

#### Interpretability
Current interpretability research is lagging far behind capabilities development. Researchers are working on understanding systems much smaller than GPT-2, while GPT-4 already exists <a class="yt-timestamp" data-t="01:00:38">[01:00:38]</a> <a class="yt-timestamp" data-t="01:03:22">[01:03:22]</a>.
*   Current interpretability successes are likened to figuring out trivial facts (e.g., where an AI stores "Eiffel Tower is in France"), which he dismisses as "1956 shit" <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>.
*   He suggests that even if significant funding (e.g., $100 billion in prizes) were directed to interpretability, and it yielded understanding of GPT-4, this knowledge might dangerously reveal how to rebuild it much smaller <a class="yt-timestamp" data-t="01:02:12">[01:02:12]</a> <a class="yt-timestamp" data-t="01:03:39">[01:03:39]</a>.

#### Legibility of AI Thought
Yudkowsky is skeptical that LLMs producing output token-by-token makes their thinking inherently legible or safe <a class="yt-timestamp" data-t="00:49:48">[00:49:48]</a>.
*   He references Ilya Sutskever's point that predicting the next token requires predicting the world that generates the token, implying sophisticated internal modeling and planning capabilities <a class="yt-timestamp" data-t="00:53:09">[00:53:09]</a> - <a class="yt-timestamp" data-t="00:54:06">[00:54:06]</a>.
*   MIRI (Machine Intelligence Research Institute) funded a "Visible Thoughts Project" to encourage LLMs to "think out loud," indicating this was seen as a small potential ray of hope, but not a comprehensive solution <a class="yt-timestamp" data-t="00:51:45">[00:51:45]</a>.

### Proposed (Hail Mary) Paths Forward

Given his pessimism, Yudkowsky outlines potential "Hail Mary passes" for humanity's survival.

#### Human Intelligence Enhancement
This is presented as a primary, albeit dangerous, alternative to developing superintelligent AI. This concept ties into the idea of [[human_enhancement_and_intelligence_augmentation]].
*   The goal is to make humans smarter so they can solve AI alignment or other existential problems <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a> <a class="yt-timestamp" data-t="00:06:42">[00:06:42]</a>.
*   Potential methods include:
    *   Neurofeedback (e.g., using MRIs to train people to be saner, rationalize less) <a class="yt-timestamp" data-t="00:07:52">[00:07:52]</a>.
    *   Using current AI (e.g., GPT-4 level) to study biology, genomics, and proteinomics to find ways to enhance adult intelligence (as there's no time to raise enhanced children from scratch) <a class="yt-timestamp" data-t="00:42:03">[00:42:03]</a>. This would involve a carve-out for AI used strictly for biology, not trained on general internet text <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>.
    *   Brain slicing, scanning, simulation, and upgrading uploads <a class="yt-timestamp" data-t="00:08:42">[00:08:42]</a>.
*   Yudkowsky acknowledges these paths are also dangerous, but not with the "utter lethality" of general AI <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>.
*   Another, less direct, form of enhancement might involve using AI like GPT-4, fine-tuned to be "consistently smart, nice and charitable," to spread sanity online, though he worries this isn't the most profitable use <a class="yt-timestamp" data-t="00:08:15">[00:08:15]</a>.

#### Global Shutdown and Regulation
A moratorium or shutdown requires a viable "exit plan" to be sustainable and effective <a class="yt-timestamp" data-t="01:35:39">[01:35:39]</a>.
*   The required duration of the shutdown depends on the speed of the exit plan (e.g., human intelligence enhancement). A 5-year plan is more manageable than a 15-year one <a class="yt-timestamp" data-t="01:35:46">[01:35:46]</a> <a class="yt-timestamp" data-t="01:36:06">[01:36:06]</a>.
*   A long-term shutdown would necessitate increasingly stringent controls, such as shutting down AI research journals and eventually confiscating home GPUs, to counteract improving algorithms that get more capability from less compute <a class="yt-timestamp" data-t="01:36:14">[01:36:14]</a> - <a class="yt-timestamp" data-t="01:37:02">[01:37:02]</a>. These strategies relate to discussions on [[challenges_in_ai_governance | challenges in AI governance]].
*   He draws a contrast with US-Soviet nuclear cooperation, which was successful because the catastrophic effects of nuclear exchange were legible (Hiroshima, Nagasaki) and escalation pathways were understood. AI, he argues, is like nuclear weapons that "spit up gold until they get too large and then ignite the atmosphere," with the ignition point being unpredictable <a class="yt-timestamp" data-t="01:30:49">[01:30:49]</a> - <a class="yt-timestamp" data-t="01:33:39">[01:33:39]</a>. This historical analogy draws parallels to [[comparisons_between_atomic_bomb_development_and_modern_ai_advancements | comparisons between atomic bomb development and modern AI advancements]].

### Yudkowsky's Overall Outlook: Profound Pessimism

Yudkowsky expresses a deeply pessimistic view, stating, "I think we are all going to die" <a class="yt-timestamp" data-t="00:07:15">[00:07:15]</a>.
*   He estimates the probability of humanity's survival as effectively 0%, stating that even an order of magnitude increase from 0% is still 0% <a class="yt-timestamp" data-t="00:16:09">[00:16:09]</a> <a class="yt-timestamp" data-t="00:21:09">[00:21:09]</a>. Later, he clarifies this means "rounding to the nearest number, there’s basically a 0% chance humanity survives" <a class="yt-timestamp" data-t="00:50:54">[00:50:54]</a>.
*   He believes that most potential paths forward, including those involving AI assistance, are fraught with insurmountable alignment challenges <a class="yt-timestamp" data-t="01:13:34">[01:13:34]</a>.
*   He views optimists in the field as often enacting a "ritual of the young optimistic scientist" who hasn't yet been "slapped down by harsh reality" <a class="yt-timestamp" data-t="01:52:22">[01:52:22]</a>.
*   He acknowledges that the future is hard to predict in detail, but argues that the *endpoint* (convergence to doom) is easier to predict than the specific pathway, much like it's easier to predict a superintelligent chess AI will win than to predict its exact moves <a class="yt-timestamp" data-t="03:30:53">[03:30:53]</a> <a class="yt-timestamp" data-t="03:31:09">[03:31:09]</a>. His pessimism stems from applying a maximum entropy distribution over what he considers the correct "space of possibilities," which to him, overwhelmingly points to outcomes where humans do not survive <a class="yt-timestamp" data-t="02:25:54">[02:25:54]</a> <a class="yt-timestamp" data-t="03:34:46">[03:34:46]</a>.
*   Even if a path with a >1% or >10% technical feasibility for survival exists (e.g., carefully managed AI for human augmentation after a global shutdown), he is not optimistic that humanity will choose or successfully navigate such a path <a class="yt-timestamp" data-t="02:57:01">[02:57:01]</a> - <a class="yt-timestamp" data-t="03:01:22">[03:01:22]</a>.