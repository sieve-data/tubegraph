---
title: Conditional music generation
videoId: SodPUNBFeMY
---

From: [[hu-po]] <br/> 

Conditional music generation is the task of creating musical pieces based on specified inputs, such as text descriptions or melodic features <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a> <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>. Recent advancements in self-supervised audio representation learning, sequential modeling, and audio synthesis have enabled the development of such models <a class="yt-timestamp" data-t="00:11:00">[00:11:00]</a>.

## Challenges in Music Generation

Generating music is a challenging task due to several factors:
*   **Long-range sequences** Music signals require modeling long-range dependencies <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>.
*   **High sampling rates** Standard music recordings typically have sampling rates of 44 kilohertz (kHz), significantly higher than 16 kHz for speech, meaning more data points per second <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a> <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>.
*   **Complex structures** Music involves harmonies and melodies from different instruments, creating intricate compositions <a class="yt-timestamp" data-t="00:09:34">[00:09:34]</a>.
*   **Human sensitivity** Human listeners are highly sensitive to disharmony, making it difficult to generate music without melodic errors <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a> <a class="yt-timestamp" data-t="00:10:04">[00:10:04]</a>.
*   **Controllability** Music creators require the ability to control various aspects of generation, such as keys, instruments, and genres <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>. [[text_and_melody_conditioning | Text conditioning]] provides a natural language interface for this control <a class="yt-timestamp" data-t="00:10:26">[00:10:26]</a>.

## MusicGen Model

MusicGen, developed by Meta AI Research, is a model for [[text_and_melody_conditioning | conditional music generation]] <a class="yt-timestamp" data-t="00:00:44">[00:00:44]</a>. It operates over multiple streams of compressed, discrete music representations, also known as tokens <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a> <a class="yt-timestamp" data-t="00:04:47">[00:04:47]</a>. Unlike prior work that often uses cascading models, MusicGen is composed of a single-stage [[transformer_models_in_audio_generation | Transformer Language Model]] (LM) <a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a> <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>. This means it generates the final product in one inference step, avoiding multiple cascading models or hierarchical approaches <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a> <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

MusicGen is available with a GitHub repository called `audiocraft`, a Colab notebook, and a Hugging Face demo <a class="yt-timestamp" data-t="00:01:25">[00:01:25]</a> <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>. It uses PyTorch 2.0 and Python 3.9 <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>.

### Audio Tokenization
To make audio modeling tractable, MusicGen represents audio signals as multiple streams of discrete tokens <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a>. It uses EnCodec, a convolutional autoencoder, which quantizes the latent space using [[code_book_patterns | Residual Vector Quantization (RVQ)]] <a class="yt-timestamp" data-t="00:29:46">[00:29:46]</a> <a class="yt-timestamp" data-t="00:31:54">[00:31:54]</a>.
*   **Sampling Rate Reduction:** EnCodec reduces the high audio sampling rate (e.g., 32 kHz) to a much lower frame rate for tokens (e.g., 50 Hz), significantly decreasing the amount of data to process <a class="yt-timestamp" data-t="00:33:41">[00:33:41]</a> <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>.
*   **Multiple Codebooks:** RVQ results in multiple parallel discrete token sequences (K codebooks), where each subsequent codebook quantizes the residual error of the previous one <a class="yt-timestamp" data-t="00:25:40">[00:25:40]</a> <a class="yt-timestamp" data-t="00:26:12">[00:26:12]</a> <a class="yt-timestamp" data-t="00:36:00">[00:36:00]</a>. The first codebook typically contains the most significant portion of the signal <a class="yt-timestamp" data-t="00:36:30">[00:36:30]</a>. MusicGen uses four quantizers, or codebooks, each with a size of 2048 <a class="yt-timestamp" data-t="01:28:53">[01:28:53]</a> <a class="yt-timestamp" data-t="01:29:00">[01:29:00]</a>.

### Codebook Patterns and Interleaving
A core contribution of MusicGen is its use of efficient [[code_book_patterns | code book interleaving patterns]] to handle the multiple parallel streams of acoustic tokens <a class="yt-timestamp" data-t="00:31:29">[00:31:29]</a> <a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a>.
*   **The Problem:** While RVQ's codebooks are generally dependent on each other, processing them sequentially (e.g., predicting one, then feeding its output to predict the next) increases computational complexity <a class="yt-timestamp" data-t="00:46:03">[00:46:03]</a> <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>. Doing them entirely in parallel loses information from dependencies <a class="yt-timestamp" data-t="00:53:15">[00:53:15]</a>.
*   **Interleaving Solutions:** MusicGen introduces and explores various [[code_book_patterns | code book patterns]] to optimize this trade-off:
    *   **Flattening Pattern:** Concatenates all codebook tokens into one large sequence for a single prediction step <a class="yt-timestamp" data-t="00:45:09">[00:45:09]</a> <a class="yt-timestamp" data-t="00:50:06">[00:50:06]</a>. This achieves the best quality but has a high computational cost due to increased autoregressive steps <a class="yt-timestamp" data-t="01:59:56">[01:59:56]</a> <a class="yt-timestamp" data-t="02:00:39">[02:00:39]</a>.
    *   **Delay Pattern:** Introduces an offset between codebooks, predicting the more important first codebook, and then shifting the less important subsequent codebooks to allow for parallel processing <a class="yt-timestamp" data-t="00:30:29">[00:30:29]</a> <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a> <a class="yt-timestamp" data-t="01:00:51">[01:00:51]</a> <a class="yt-timestamp" data-t="01:03:13">[01:03:13]</a>. This significantly reduces autoregressive steps (e.g., 1500 steps for 30 seconds of audio compared to 6000 for flattening) <a class="yt-timestamp" data-t="01:59:58">[01:59:58]</a> <a class="yt-timestamp" data-t="02:00:28">[02:00:28]</a>.
    *   **Partial Flattening / Partial Delay:** Similar to Delay, but involves interleaving codebooks, leading to more steps than Delay but fewer than full Flattening <a class="yt-timestamp" data-t="02:03:57">[02:03:57]</a>.
    *   **Valley Pattern:** Predicts the first codebook sequentially for all time steps, then predicts all remaining codebooks in parallel <a class="yt-timestamp" data-t="02:03:39">[02:03:39]</a>.

The model uses a [[transformer_models_in_audio_generation | Transformer decoder]] with L layers and dimension D, incorporating causal self-attention blocks and cross-attention blocks that are fed the conditioning signal <a class="yt-timestamp" data-t="01:15:57">[01:15:57]</a> <a class="yt-timestamp" data-t="01:16:13">[01:16:13]</a>.

### Conditioning Methods
MusicGen supports conditioning on two modalities:
1.  **Textual Descriptions:** The model can be conditioned on textual descriptions of the desired music <a class="yt-timestamp" data-t="01:43:52">[01:43:52]</a>. For this, it uses pre-trained text encoders like T5, Flan-T5, and CLAP <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a> <a class="yt-timestamp" data-t="01:07:05">[01:07:05]</a> <a class="yt-timestamp" data-t="01:08:16">[01:08:16]</a>. Text normalization (omitting stop words, lemmatization) and word dropout are used as text augmentation strategies <a class="yt-timestamp" data-t="01:40:01">[01:40:01]</a> <a class="yt-timestamp" data-t="01:40:23">[01:40:23]</a>.
2.  **Melody Conditioning:** MusicGen can also be conditioned on an input melody (e.g., an MP3 file) <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a> <a class="yt-timestamp" data-t="01:08:51">[01:08:51]</a>. This is achieved by computing a chromogram from the melody, which is a 2D image-like representation of pitch class over time <a class="yt-timestamp" data-t="01:09:11">[01:09:11]</a> <a class="yt-timestamp" data-t="01:09:22">[01:09:22]</a>. To create an information bottleneck, the model selects the dominant time-frequency bin (using ARG Max) in each step of the chromogram, turning it into a 1D sequence that can be prefixed to the Transformer input <a class="yt-timestamp" data-t="01:10:26">[01:10:26]</a> <a class="yt-timestamp" data-t="01:18:24">[01:18:24]</a>. This approach is unsupervised, eliminating the need for costly supervised data <a class="yt-timestamp" data-t="01:10:59">[01:10:59]</a>.

## Training and Evaluation

MusicGen was trained on 20,000 hours of licensed music, including an internal dataset from Meta and data from Shutterstock and Pond5 music <a class="yt-timestamp" data-t="01:43:17">[01:43:17]</a>. The model processes 30-second audio crops for 1 million steps using an AdamW optimizer <a class="yt-timestamp" data-t="01:34:06">[01:34:06]</a> <a class="yt-timestamp" data-t="01:34:10">[01:34:10]</a>.

Evaluation involved both objective metrics and human studies on the MusicCaps benchmark <a class="yt-timestamp" data-t="01:44:37">[01:44:37]</a> <a class="yt-timestamp" data-t="01:47:26">[01:47:26]</a>.
*   **Objective Metrics:** Fréchet Audio Distance (FAD), KL Divergence (for audio classification probabilities), and CLAP score (for audio-text alignment) <a class="yt-timestamp" data-t="01:48:42">[01:48:42]</a> <a class="yt-timestamp" data-t="01:50:01">[01:50:01]</a> <a class="yt-timestamp" data-t="01:51:50">[01:51:50]</a>.
*   **Human Studies:** Raiders rated overall quality and relevance to text input on a scale of 1 to 100 using Amazon Mechanical Turk <a class="yt-timestamp" data-t="01:53:33">[01:53:33]</a> <a class="yt-timestamp" data-t="01:53:39">[01:53:39]</a>. A new metric, chroma cosine similarity, was introduced to measure melody adherence <a class="yt-timestamp" data-t="01:59:52">[01:59:52]</a>.

### Key Findings
*   **Performance:** MusicGen generally performs better than re-implemented baselines (e.g., Riffusion, Moûsaï) <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a>.
*   **Melody Conditioning:** While objective metrics may degrade slightly with [[text_and_melody_conditioning | melody conditioning]], human ratings are not significantly affected <a class="yt-timestamp" data-t="01:58:30">[01:58:30]</a>. However, it successfully helps the model follow a given melody <a class="yt-timestamp" data-t="01:59:37">[01:59:37]</a>.
*   **Codebook Patterns:** Flattening offers the best scores but at a high computational cost. Delay and partial flattening achieve similar performance with significantly fewer steps <a class="yt-timestamp" data-t="01:59:51">[01:59:51]</a> <a class="yt-timestamp" data-t="02:00:28">[02:00:28]</a>. The choice often comes down to a trade-off between quality and performance <a class="yt-timestamp" data-t="02:24:15">[02:24:15]</a>.
*   **Model Size:** Scaling up the model size (e.g., from 300 million to 3.3 billion parameters) results in only marginally better scores, with human evaluators often unable to discern the difference <a class="yt-timestamp" data-t="02:01:11">[02:01:11]</a> <a class="yt-timestamp" data-t="02:01:27">[02:01:27]</a>. This suggests that 300 million parameters may be sufficient for high-quality generation <a class="yt-timestamp" data-t="02:01:55">[02:01:55]</a>.
*   **Text Encoders:** The choice of text encoder (e.g., T5 vs. Flan-T5) appears to have a notable impact on performance, potentially more so than other architectural or pattern choices <a class="yt-timestamp" data-t="02:15:31">[02:15:31]</a> <a class="yt-timestamp" data-t="02:16:01">[02:16:01]</a>.

## Ethical Considerations
The authors acknowledge several ethical challenges:
*   **Data Set Diversity:** The training data contains a large proportion of Western-style music, potentially leading to a lack of diversity in generated outputs <a class="yt-timestamp" data-t="02:12:23">[02:12:23]</a>.
*   **Competition for Artists:** Generative models could represent unfair competition for human artists <a class="yt-timestamp" data-t="02:12:31">[02:12:31]</a>.
*   **Access:** Open research is emphasized to ensure equal access to these models for all actors <a class="yt-timestamp" data-t="02:12:42">[02:12:42]</a>.

## Related Work
The field of music generation has evolved significantly, with various approaches:
*   **Compressed Representation:** A prominent approach involves representing music as compressed symbols or tokens, then applying generative models <a class="yt-timestamp" data-t="02:05:35">[02:05:35]</a>. This includes using [[code_book_patterns | VQVAE]] on raw waveforms with [[code_book_patterns | Residual Vector Quantization]] <a class="yt-timestamp" data-t="02:06:00">[02:06:00]</a>.
*   **Symbolic Music Generation:** Earlier work included generating MIDI files, though raw audio waveform generation is now more common <a class="yt-timestamp" data-t="02:06:33">[02:06:33]</a> <a class="yt-timestamp" data-t="02:27:44">[02:27:44]</a>.
*   **Multiple Streams:** Some models represent music in multiple streams of discrete representations, such as semantic and acoustic tokens for speech <a class="yt-timestamp" data-t="02:06:58">[02:06:58]</a> <a class="yt-timestamp" data-t="02:07:09">[02:07:09]</a>.
*   **Diffusion Models:** An alternative approach uses diffusion models, which operate over continuous representations, unlike the discrete tokens used by MusicGen <a class="yt-timestamp" data-t="02:07:50">[02:07:50]</a>. Some work has fine-tuned [[stateoftheart_video_generation_and_multimodal_models | Stable Diffusion]] to generate spectrograms, which are then converted to audio <a class="yt-timestamp" data-t="02:08:26">[02:08:26]</a>.
*   **Conditional Computation:** Other models, such as MusicLM, use hierarchical or cascading approaches, where one model generates coarse audio, and another refines it <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a> <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a> <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a> <a class="yt-timestamp" data-t="00:46:55">[00:46:55]</a>.