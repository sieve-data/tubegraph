---
title: Lessons learned from catastrophic software failures
videoId: Iq_r7IcNmUk
---

From: [[fireship]] <br/> 

Software is an integral part of modern life, but even the smallest bugs can lead to significant consequences, ranging from minor annoyances to widespread disasters and even loss of life. These incidents serve as critical lessons in the importance of robust software development, thorough testing, and careful deployment. The deeper a bug is within a system, the more severe its potential impact <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>.

## Minor Impact and Unforeseen Features

Not all "bugs" are detrimental; sometimes, they become beloved "features" <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. A famous example is the "Gandhi bug" in the original *Sid Meier's Civilization* game. Due to Gandhi's aggression level being set to an unsigned integer of one, adopting diplomacy (which reduced aggression by two) caused an unsigned integer underflow, maxing out his aggression to 255 and turning him into a "thermonuclear Enthusiast" <a class="yt-timestamp" data-t="00:00:07">[00:00:07]</a>. This bug was so popular it became an urban legend and a feature in later games <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a>.

However, real-world software glitches often have tangible negative consequences <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>.

### Case Studies: Tier 1 - Everyday Disruptions

*   **Microsoft Zune Freeze (2008)**: Every Zune device worldwide froze on December 31st, turning into a "stylish brick" <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. The issue stemmed from the device not being programmed to account for the 366th day of a leap year, leading to a logic error that caused the software to get stuck in a loop <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. This could have been fixed by a simple `break` statement <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a>.
*   **Pentium FPU Bug (1994)**: Occasionally, when performing floating-point division, Intel's Pentium chip would return an incorrect value <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. This extremely rare error (about 1 in 9 billion operations) was discovered by a professor <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>. The root cause was five missing entries in a lookup table used by the SRT division algorithm at the hardware level <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>.
*   **Apple FaceTime Eavesdropping Bug (2019)**: A 14-year-old discovered a bug allowing users to eavesdrop on recipients before a FaceTime call was answered, and even activate their camera if the power button was pressed <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. This occurred because the system was not checking the call's state before activating the audio stream <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. Apple temporarily disabled group FaceTime and later issued a patch, awarding the discoverer a bug bounty <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>.
*   **Chase ATM Glitch (2024)**: Due to a glitch in JP Morgan Chase's banking system, likely running on old proprietary code, a safeguard was prevented, allowing people to withdraw funds immediately after depositing fake checks <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>. While this seemed like a windfall, it constituted check fraud, leading to negative bank balances and legal action for those involved <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>. [[consequences_of_software_glitches_in_financial_systems | Consequences of software glitches in financial systems]]

## Wider Disruptions and Economic Impact

As software complexity increases, so does the potential for widespread disruption.

### Case Studies: Tier 2 - Systemic Failures

*   **AT&T Long-Distance Crash (1990)**: A single faulty line of C code in a software update caused a network switch to crash and reboot. This triggered a cascading failure, as the rebooting switch caused neighboring switches to fail, taking out the entire long-distance network and blocking 50 million calls globally <a class="yt-timestamp" data-t="00:04:37">[00:04:37]</a>. The bug involved a `break` statement within an `if` clause inside a `switch` clause, leading to premature data overwrites <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>. [[impact_of_faulty_software_updates_on_global_systems | Impact of faulty software updates on global systems]]
*   **F-35 Oxygen System Bug (2012)**: Pilots experienced hypoxia-like symptoms due to the On-Board Oxygen Generation System (OBOGS) <a class="yt-timestamp" data-t="00:05:23">[00:05:23]</a>. The underlying software was not robust enough to account for real-time variables like rapid altitude changes or pilot breathing patterns, leading to insufficient oxygen delivery <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>. This highlights the critical need for thorough testing under all possible conditions when dealing with life-support systems <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.
*   **Heathrow Terminal 5 Baggage System (2008)**: The opening of the £4.3 billion Heathrow Terminal 5 was plagued by a software bug, leading to over 500 canceled flights and 42,000 lost bags <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>. Multiple software systems failed to communicate, preventing employees from logging in and tracking bags, resulting in a system breakdown <a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>. This was a costly example of [[challenges_with_automated_software_updates_and_system_safety | testing in production]] gone wrong <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>. [[impact_of_software_bugs_on_different_industries | Impact of software bugs on different industries]]
*   **Vancouver Stock Exchange (1982)**: The value of the Vancouver Stock Exchange slowly decreased over two years, dropping from 1,000 to 520, due to a rounding error <a class="yt-timestamp" data-t="00:06:42">[00:06:42]</a>. The software truncated each stock price change to two decimal places instead of rounding, creating a cumulative error over time <a class="yt-timestamp" data-t="00:07:03">[00:07:03]</a>. The index had to be completely recalculated <a class="yt-timestamp" data-t="00:07:14">[00:07:14]</a>.
*   **Morris Worm (1988)**: Robert Morris, a Cornell graduate student, released a self-replicating computer program to gauge the internet's size <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>. A bug in the code caused it to reinfect the same computer repeatedly, overwhelming resources and crashing thousands of Unix-based systems <a class="yt-timestamp" data-t="00:07:59">[00:07:59]</a>. It took out 6,000 computers, approximately 10% of the internet at the time, leading to the first felony conviction under the Computer Fraud and Abuse Act <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>. [[famous_software_bugs_in_history | Famous software Bugs in History]]

## Financial Losses and Direct Dangers

Software bugs can lead to colossal financial losses and directly endanger human lives.

### Case Studies: Tier 3 - Critical Errors

*   **Mars Climate Orbiter (1999)**: The $125 million NASA Mars Climate Orbiter burned up upon entering Mars's atmosphere because one software team used Imperial units (pound-force) while another used metric units (Newton-seconds) <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>. This unit mismatch caused a fatal navigation error <a class="yt-timestamp" data-t="00:08:23">[00:08:23]</a>.
*   **Ariane 5 Rocket Explosion (1996)**: The Ariane 5 rocket exploded 37 seconds after liftoff, destroying over $370 million of equipment <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>. A software bug in the inertial reference system incorrectly converted a 64-bit floating-point number to a 16-bit integer. This led the rocket to believe it was 90 degrees off course, causing it to self-destruct during high-velocity correction <a class="yt-timestamp" data-t="00:08:40">[00:08:40]</a>. [[famous_software_bugs_in_history | Famous software Bugs in History]]
*   **Toyota Prius Braking System (2010)**: A bug in the anti-lock braking system caused momentary delays when switching from regenerative braking to friction braking, particularly on icy roads or potholes <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a>. This 0.4-second delay led to a recall of 400,000 vehicles <a class="yt-timestamp" data-t="00:09:12">[00:09:12]</a>.
*   **Citibank Bad UI Disaster (2020)**: Citibank accidentally transferred the full loan amount of $900 million instead of an $8 million interest payment <a class="yt-timestamp" data-t="00:09:30">[00:09:30]</a>. The software had a confusing three-screen payment process, and the interface's checkboxes that *appeared* to ensure only interest was paid actually did the opposite <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>. A court later ruled in favor of the lenders, allowing them to keep the accidental transfer <a class="yt-timestamp" data-t="00:09:58">[00:09:58]</a>. [[consequences_of_software_glitches_in_financial_systems | Consequences of software glitches in financial systems]]
*   **Y2K Bug (2000)**: Early programmers used only two digits to represent years, leading to fears that computers would interpret '00' as 1900 instead of 2000 <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>. While media hysteria led to billions of dollars spent in preparation, the widespread disasters predicted largely did not materialize due to extensive preventative measures <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a>.

## Catastrophic Worldwide Damage and Fatalities

The most critical bugs can cause widespread system failures, economic devastation, and even death.

### Case Studies: Tier 4 - Global Catastrophes

*   **Knight Capital Money Burn (2012)**: Knight Capital, an algorithmic trading firm, lost $440 million in 45 minutes, wiping out 75% of investors' equity <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>. Developers accidentally linked a variable name to an outdated testing algorithm ("Power Peg") designed to manipulate virtual markets by buying high and selling low <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>. When pushed to production, it flooded the New York Stock Exchange with 4 million incorrect trades <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>.
*   **Heartbleed Vulnerability (2014)**: A simple coding oversight – a missing bounds check – in OpenSSL, a library critical for internet communication security, created the Heartbleed vulnerability <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>. Improper input validation allowed attackers to repeatedly request memory contents from a server, potentially gaining access to confidential data undetected <a class="yt-timestamp" data-t="00:11:41">[00:11:41]</a>. Two-thirds of internet servers were vulnerable <a class="yt-timestamp" data-t="00:11:55">[00:11:55]</a>.
*   **Toyota Unintended Acceleration (2009-2010)**: This issue led to 6,200 complaints, 52 injuries, and 89 deaths <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>. While a multitude of issues were involved, a software bug in the electronic throttle control system was partly to blame <a class="yt-timestamp" data-t="00:12:13">[00:12:13]</a>. The code had terrible error handling logic and lacked redundancy, with multiple failure points that could cascade through the system <a class="yt-timestamp" data-t="00:12:24">[00:12:24]</a>. Toyota recalled 9 million vehicles and paid $1.2 billion in fines <a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>.
*   **CrowdStrike Y2K (2024)**: Cybersecurity company CrowdStrike pushed a bad configuration file, "Channel file 291," to production <a class="yt-timestamp" data-t="00:12:42">[00:12:42]</a>. This resulted in millions of Windows machines experiencing the Blue Screen of Death, leading to hospitals shutting down, flights being canceled, and widespread disruption <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>. [[impact_of_faulty_software_updates_on_global_systems | Impact of faulty software updates on global systems]]
*   **Northeastern Blackout (2003)**: Nearly 50 million people in the U.S. and Canada lost power <a class="yt-timestamp" data-t="00:13:09">[00:13:09]</a>. First Energy Corporation's power grid monitoring system had poor error handling; if multiple alarms were generated quickly, the system would enter an unrecoverable state without notifying operators <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>. This meant operators saw a normal display while half the country was without power <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>. [[vulnerability_of_critical_infrastructures_to_software_errors | Vulnerability of critical infrastructures to software errors]]

### Case Studies: Tier 5 - Deadly Consequences

*   **Royal Air Force Helicopter Crash (1994)**: A military helicopter crashed in foggy conditions, killing 25 people <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>. The automatic throttle control system became overloaded under challenging conditions and didn't know how to respond <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>. The investigation found the software was not adequately tested for these specific environmental conditions <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a>.
*   **Therac-25 Radiation Machine (1985-1987)**: This device, used to treat cancer patients, had inadequate error handling and a race condition bug <a class="yt-timestamp" data-t="00:14:12">[00:14:12]</a>. When triggered, it would deliver lethal radiation doses, up to 100 times higher than intended <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>. At least three patients died <a class="yt-timestamp" data-t="00:14:30">[00:14:30]</a>. A fatal mistake was the removal of mechanical interlocks, relying entirely on software for safety <a class="yt-timestamp" data-t="00:14:36">[00:14:36]</a>. [[vulnerability_of_critical_infrastructures_to_software_errors | Vulnerability of critical infrastructures to software errors]]
*   **Patriot Missile System (1991)**: During the Gulf War, a Scud missile killed 28 American soldiers after it wasn't intercepted by the Patriot missile system <a class="yt-timestamp" data-t="00:14:51">[00:14:51]</a>. A bug in the system's clock and timing calculations caused a 24-bit timer to overflow after 100 hours of operation, leading to incorrect information about incoming threats <a class="yt-timestamp" data-t="00:14:58">[00:14:58]</a>.
*   **Aegis Combat System (1988)**: This system accidentally shot down a civilian Iranian plane, killing 290 people <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>. The investigation revealed a lack of user-friendly information on the system's display and a timing lag that produced misleading altitude data, leading to the misidentification of a friendly plane as a threat <a class="yt-timestamp" data-t="00:15:28">[00:15:28]</a>.
*   **Boeing 737 Max MCAS (2018-2019)**: The Maneuvering Characteristics Augmentation System (MCAS) was designed to automatically push the nose down to prevent stalls <a class="yt-timestamp" data-t="00:15:53">[00:15:53]</a>. A major programming oversight allowed the system to initiate this sequence if only one of two angle of attack sensors provided faulty data <a class="yt-timestamp" data-t="00:16:01">[00:16:01]</a>. This led to two tragic flights, Lion Air Flight 610 and Ethiopian Airlines Flight 302, killing 346 passengers and crew <a class="yt-timestamp" data-t="00:16:15">[00:16:15]</a>. Boeing's 737 Max planes were grounded globally <a class="yt-timestamp" data-t="00:16:24">[00:16:24]</a>. The fix involved ensuring both sensors provided consistent data before nose-down commands <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.

While writing entirely bug-free code is challenging, these events underscore the critical importance of rigorous testing, clear specifications, robust error handling, and user-centered design, especially in systems with high stakes. [[common_frustrations_in_software_engineering | Common frustrations in software engineering]] [[challenges_of_finishing_software_projects | Challenges of finishing software projects]] [[crisis_management_and_recovery_strategies_after_it_failures | Crisis management and recovery strategies after IT failures]]