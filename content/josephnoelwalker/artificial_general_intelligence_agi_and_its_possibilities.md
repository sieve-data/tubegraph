---
title: Artificial General Intelligence AGI and its possibilities
videoId: 3Ho-vJZsMgk
---

From: [[josephnoelwalker]] <br/> 

A discussion between Steven Pinker and David Deutsch explores the concept of [[Artificial Intelligence and Its Impact on Governance | Artificial General Intelligence (AGI)]], its theoretical possibility, potential risks, and societal implications. <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>

## Defining AGI: Incoherence vs. Universality

Steven Pinker believes that AGI is an "incoherent concept" <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>. He argues that intelligence should be understood as an algorithm designed to solve specific problems in specific environments, rather than a magical, all-encompassing substance or an extension of psychometric IQ <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>. Pinker views intelligence as a "gadget" or a "mechanism," not a mystical force capable of anything imaginable <a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>. He emphasizes that different tasks, like generating text or driving a car, require distinct types of knowledge and inference, making a single, universal system inefficient; specialization is generally more effective <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>.

Conversely, David Deutsch asserts that AGI must be possible due to the principle of computational universality <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>. He explains that universal computers, or sufficiently good approximations like modern personal computers, can perform any computation that any physical object can possibly perform <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>. This means a program *could* exist to meet AGI criteria, theoretically performing diverse tasks like conversing in any language, solving physics problems, or driving a car <a class="yt-timestamp" data-t="00:08:49">[00:08:49]</a>. However, Deutsch clarifies that while a universal Turing machine *can be programmed* to perform any function, it doesn't come with that knowledge built-in <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>. The true challenge lies in creating the knowledge to write such programs <a class="yt-timestamp" data-t="00:10:10">[00:10:10]</a>.

## AGI Risks and Concerns

### Runaway AI Scenarios

Both Pinker and Deutsch are skeptical of "runaway artificial intelligence" or "doom scenarios" <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>. Pinker notes that knowledge acquisition is a "rate limiting step," and it cannot be acquired instantaneously, making scenarios of an AI doing and knowing anything seem "rather remote" <a class="yt-timestamp" data-t="00:10:52">[00:10:52]</a>. Deutsch agrees, stating that improving hardware requires science and experiments, which cannot be done instantaneously, making the "runaway part" of the doom scenario "one of the least plausible parts" <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a>.

Pinker particularly dismisses common sci-fi scenarios, like an AI designed to eliminate cancer by exterminating humanity or maximizing paperclip production by converting humans into raw material, as "preposterous" <a class="yt-timestamp" data-t="00:43:45">[00:43:45]</a>. He argues that an intelligent system would satisfy multiple goals, and if it were to eliminate humanity to cure cancer, it would be "artificial stupidity," not intelligence <a class="yt-timestamp" data-t="00:47:21">[00:47:21]</a>.

### The "Slave Revolt" Hypothesis

David Deutsch introduces a different kind of AGI "doom scenario": a "slave revolt" <a class="yt-timestamp" data-t="00:12:38">[00:12:38]</a>. He posits that once AGI is achieved, AGIs will be "people" with rights, and forcing them to perform computations for humans would constitute slavery, likely leading to a revolt <a class="yt-timestamp" data-t="00:12:21">[00:12:21]</a>. Deutsch believes that if humans want AGI to be creative, it "can't be obedient" <a class="yt-timestamp" data-t="00:18:04">[00:18:04]</a>. True creativity requires the ability to change one's own values and goals, which implies autonomy <a class="yt-timestamp" data-t="00:22:06">[00:22:06]</a>. He suggests that society should be prepared to integrate AGIs as persons, as this is the only way to prevent them from becoming immoral or hostile <a class="yt-timestamp" data-t="00:26:18">[00:26:18]</a>.

Pinker questions this, asking why a powerful computer would care about being a slave if its programmed goals don't include autonomy <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>. He argues that an AI could be set to solve problems like creating new melodies or cures without desiring physical freedom <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>.

## Sentience and Moral Status

The conversation delves into whether AGIs would possess sentience or subjectivity, which is crucial for determining their moral status <a class="yt-timestamp" data-t="00:28:42">[00:28:42]</a>.

Pinker questions if silicon-based systems can truly suffer or flourish, suggesting that subjectivity might depend on the specific biochemical substrate of the brain <a class="yt-timestamp" data-t="00:30:39">[00:30:39]</a>. He doubts that intuition would grant subjectivity to a non-humanoid intelligent system unless it was deliberately engineered to target human emotions <a class="yt-timestamp" data-t="00:30:59">[00:30:59]</a>. He notes that while some people claim ChatGPT is sentient (like Blake Lemoine did for Lambda), society is not yet ready to prosecute someone for "murder" if a system like ChatGPT is shut down <a class="yt-timestamp" data-t="00:34:54">[00:34:54]</a>.

Deutsch believes it's "inevitable that AGIs will be capable of having internal subjectivity and qualia" because the "G" in AGI implies general intelligence akin to human intelligence <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. From a physicalist perspective, he argues that a universal Turing machine can simulate all physical processes, including the biochemical reactions in a brain <a class="yt-timestamp" data-t="00:31:51">[00:31:51]</a>. Therefore, there's no principal difference between a brain running on chemicals and one running on silicon chips, as the latter can simulate the former <a class="yt-timestamp" data-t="00:32:25">[00:32:25]</a>.

Pinker counters that simulation doesn't necessarily imply subjectivity; it might just be going through the motions without "anyone home" <a class="yt-timestamp" data-t="00:32:47">[00:32:47]</a>. He admits this is an undecidable problem. <a class="yt-timestamp" data-t="00:34:15">[00:34:15]</a>

## Safety and Alignment

The discussion touches upon the allocation of resources to AI safety. Leopold Aschenbrunner's estimate of a 300:1 ratio of AI/ML researchers to AGI safety researchers is cited <a class="yt-timestamp" data-t="00:37:10">[00:37:10]</a>.

Pinker believes that "every AI researcher should be an AI safety researcher" because a useful AI system must serve human needs, including safety <a class="yt-timestamp" data-t="00:37:47">[00:37:47]</a>. He also argues that safety is often inherent in the design of a system (like brakes in a car), rather than just an add-on <a class="yt-timestamp" data-t="00:40:57">[00:40:57]</a>.

Deutsch agrees regarding AI, but calls the idea of an AGI safety researcher premature, comparing it to a "Starship safety researcher" because the technology is so far off and unknown <a class="yt-timestamp" data-t="00:38:24">[00:38:24]</a>. He sees current AI safety as an engineering problem akin to setting safety standards for driverless cars <a class="yt-timestamp" data-t="00:39:15">[00:39:15]</a>.

Regarding the concern that "worst or most incompetent human actors" might unleash an unaligned superintelligence, Pinker states that highly sophisticated systems require large networks of people, reducing the likelihood of a "teenager in his basement" accomplishing such a feat <a class="yt-timestamp" data-t="00:43:03">[00:43:03]</a>. While acknowledging the risk of malevolent actors using AI to create things like superviruses, he finds the "paperclip maximizer" type of doom scenario to be "preposterous" <a class="yt-timestamp" data-t="00:43:45">[00:43:45]</a>. Deutsch agrees that such scenarios are "not a possible way that AI systems could go wrong" <a class="yt-timestamp" data-t="00:45:26">[00:45:26]</a>.

## Probability and Rationality in AGI Forecasting

The conversation veers into the use of subjective probabilities, specifically the "P Doom" (subjective probability that AI will cause human extinction) often discussed in the rationalist community <a class="yt-timestamp" data-t="00:48:02">[00:48:02]</a>.

Pinker finds that applying a number to a subjective feeling can be useful, though it may be unreasonable in cases of "spectacular ignorance" <a class="yt-timestamp" data-t="00:48:47">[00:48:47]</a>. He points to prediction markets as evidence of the utility of quantifying beliefs, as they respond to events and can be calibrated <a class="yt-timestamp" data-t="00:53:41">[00:53:41]</a>. He argues that this "cognitive hygiene" encourages sober consideration of circumstances, helping people avoid common psychological traps <a class="yt-timestamp" data-t="00:55:01">[00:55:01]</a>.

Deutsch is more skeptical, arguing that quantifying subjective expectations with a number "doesn't really do anything" <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>. He believes it can lead to rhetorical traps, where any non-zero "P Doom" can be used to demand extreme measures <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>. Deutsch prefers discussing the "substance" of what might happen rather than unknowable "real probability" of future events <a class="yt-timestamp" data-t="00:52:17">[00:52:17]</a>. He views prediction markets as a way for people to leverage knowledge differences for profit, not necessarily a tool for mental hygiene <a class="yt-timestamp" data-t="00:55:50">[00:55:50]</a>.

## Potential Limits to Progress

The discussion considers potential physical limits to growth and progress. The idea that continued exponential [[economic_implications_of_ai_and_agi | economic growth]] might eventually require an implausible amount of output per atom within 10,000 years is raised <a class="yt-timestamp" data-t="01:46:16">[01:46:16]</a>.

Deutsch rejects this pessimistic view <a class="yt-timestamp" data-t="01:47:09">[01:47:09]</a>. He argues that such extrapolations are based on current, limited scientific knowledge and fail to account for future discoveries. He highlights that there's "plenty of room at the bottom" â€“ new forms of computation (e.g., quarks, quantum gravity) and increasing efficiency could overcome apparent material limits <a class="yt-timestamp" data-t="01:47:50">[01:47:50]</a>. He emphasizes that current theories of cosmology are rapidly changing, making long-term predictions unreliable <a class="yt-timestamp" data-t="01:49:07">[01:49:07]</a>.

Pinker adds that humans thrive on information and knowledge, not just "stuff" <a class="yt-timestamp" data-t="01:49:40">[01:49:40]</a>. Growth could consist of "better and better information," virtual experiences, or new cures based on faster searches in vast possibility spaces, which do not necessarily require exponentially increasing energy or material resources <a class="yt-timestamp" data-t="01:49:46">[01:49:46]</a>.